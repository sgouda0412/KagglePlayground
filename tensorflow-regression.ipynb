{"cells":[{"metadata":{"_cell_guid":"3b1c110f-4fa4-4244-becb-0ba79b054c5e","_uuid":"f6922a2a6e1030b76e5cda1ff2d86bfa17708baa"},"cell_type":"markdown","source":"## trying some TensorFlow model to get used to with it\n\nThis notebook is inspired by that of Julien Heiduk https://www.kaggle.com/zoupet/neural-network-model-for-house-prices-tensorflow, thanks to him, and google machine learning mooc\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport itertools\nimport tensorflow as tf\nimport pandas as pd\nfrom tensorflow.python.data import Dataset\n\nimport numpy as np\nimport math\nfrom sklearn.ensemble import IsolationForest\nfrom pylab import rcParams\nimport matplotlib\n\nfrom scipy import stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats.stats import pearsonr\nimport seaborn as sns\nmycols = [\"#66c2ff\", \"#5cd6d6\", \"#00cc99\", \"#85e085\", \"#ffd966\", \"#ffb366\", \"#ffb3b3\", \"#dab3ff\", \"#c2c2d6\"]\nsns.set_palette(palette = mycols, n_colors = 4)\n\nfrom IPython import display\n\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\nfull_df = pd.concat((train,test))","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"d7c89d9e-54bf-49ed-89c4-8e7838355135","_uuid":"61ce361955ee7a41e12624ca33f5a1383b76eca2","collapsed":true,"trusted":true},"cell_type":"code","source":"def fill_cat(df) :\n    for col_name in df.columns : \n        if(df[col_name].dtype == 'object') :\n            df[col_name] = df[col_name].fillna(\"NONE\")\n            \n#------------------------- fill num features----------------------------\n\n\ndef fill_numerique_value(df) : \n    for col_name in df.columns : \n        if(df[col_name].dtype != \"object\") :\n            df[col_name] = df[col_name].fillna(df[col_name].mean())\n            \n\n            \n#------------------------------- remove skew_feat ------------------------------\n\ndef skewed_feature(df, treshold = None) : \n    \n    # Check how skewed they are\n    df =  df.select_dtypes(exclude=['object'])\n    skewed_feats = df.apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n    skewness = skewed_feats[abs(skewed_feats) > 0.5]\n    skewed_features = skewness.index\n    lam = treshold\n    for col in skewed_features:\n        df[col] = boxcox1p(df[col], lam)\n    print(skewness.shape[0],  \"skewed numerical features have been Box-Cox transformed\")\n    \n#----------------------------------- transformation of numeric feature ------------------------------------    \n\ndef quadratic_function(df, column) : \n    df.column =  df.column ** 2\n\ndef cubic_function(df, column) : \n    df.column =  df.column ** 3\n    \ndef squared_function(df, column):\n    df.columns = np.sqrt(df.column)\n    \ndef linear_scale(series):\n    min_val = series.min()\n    max_val = series.max()\n    scale = (max_val - min_val) / 2.0\n    \n    return series.apply(lambda x:((x - min_val) / scale) - 1.0)\n\ndef log_normalize(series):\n    return series.apply(lambda x:math.log(x+1.0))\n\ndef clip(series, clip_to_min, clip_to_max):\n    return series.apply(lambda x:(\n        min(max(x, clip_to_min), clip_to_max)))\n\ndef z_score_normalize(series):\n    mean = series.mean()\n    std_dv = series.std()\n    return series.apply(lambda x:(x - mean) / std_dv)\n\ndef binary_threshold(series, threshold):\n    return series.apply(lambda x:(1 if x > threshold else 0))\n    \ndef apply_function(df, transformation_function) : \n    \n    df =  df.select_dtypes(exclude=['object'])\n    for col in df :\n        df[col] =  transformation_function(df[col])\n    print(\"transformation done !\")\n    ","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"ecaaf36f-3c53-45b5-aa28-6580e40cea07","_uuid":"b17db5ae651aa3f62034ee40c583af4394c3f0fd","collapsed":true,"trusted":true},"cell_type":"code","source":"# ------------------------------------- Remove the outliers ----------------------------\n\ndef remove_outliers(full_df) :\n    \n    df_numeric =  full_df.select_dtypes(exclude=[\"object\"])\n    df_cat =  full_df.select_dtypes(include=[\"object\"])\n    \n    clf = IsolationForest(max_samples = 100, random_state = 42)\n    clf.fit(df_numeric)\n    y_noano = clf.predict(df_numeric)\n    y_noano = pd.DataFrame(y_noano, columns = ['Top'])\n    y_noano[y_noano['Top'] == 1].index.values\n\n    df_numeric = df_numeric.iloc[y_noano[y_noano['Top'] == 1].index.values]\n    df_numeric.reset_index(drop = True, inplace = True)\n\n    df_cat = df_cat.iloc[y_noano[y_noano['Top'] == 1].index.values]\n    df_cat.reset_index(drop = True, inplace = True)\n\n    full_df = full_df.iloc[y_noano[y_noano['Top'] == 1].index.values]\n    full_df.reset_index(drop = True, inplace = True)\n\n    print(\"Number of Outliers:\", y_noano[y_noano['Top'] == -1].shape[0])\n    print(\"Number of rows without outliers:\", full_df.shape[0])\n","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"5ebd5ddd-8874-40f5-9177-a25d174a48ce","_uuid":"738dee93cdae99ce337c1d68f3b94050b058c6c0","trusted":true},"cell_type":"code","source":"def preprocess_features(full_df):\n\n    fill_numerique_value(full_df)\n    fill_cat(full_df) \n    skewed_feature(full_df, 0.15)\n    apply_function(full_df, linear_scale)\n    remove_outliers(full_df)\n    selected_features = full_df\n    \n    processed_features = selected_features.copy()\n    \n    return processed_features\n\ndef preprocess_target(full_df):\n    \n    output_targets = pd.DataFrame()\n    output_targets[\"SalePrice\"] = log_normalize(full_df[\"SalePrice\"])\n\n    return output_targets\n\ntraining_examples, validation_examples, training_targets, validation_targets =  train_test_split(\n    preprocess_features(full_df),preprocess_target(full_df), test_size = 1459, random_state = 42)\n    \n# Double-check that we've done the right thing.\nprint (\"Training examples summary:\")\ndisplay.display(training_examples.describe())\nprint (\"Validation examples summary:\")\ndisplay.display(validation_examples.describe())\n\nprint (\"Training targets summary:\")\ndisplay.display(training_targets.describe())\nprint (\"Validation targets summary:\")\ndisplay.display(validation_targets.describe())","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"1c00f367-59e6-477b-8a4c-1eefc48c83b9","_uuid":"0df0fe5ee09a785b4ba8fee60e9062c0106e6402","collapsed":true,"trusted":true},"cell_type":"code","source":"    # need some list of features during process\n    \n    full_num = full_df.select_dtypes(exclude=['object'])\n    full_cat = full_df.select_dtypes(include=['object'])\n    col_train_num = list(full_num.columns)\n    col_train_num_bis = list(full_num.columns)\n    col_train_cat = list(full_cat.columns)\n    col_train_num_bis.remove('SalePrice')\n    COLUMNS = col_train_num\n    FEATURES = col_train_num_bis\n    LABEL = \"SalePrice\"\n    FEATURES_CAT = col_train_cat","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"317efb87-5d63-4839-810e-25356541996a","_uuid":"926e41e2e9060ce93db9c46aeed4f661b8b487e3","collapsed":true,"trusted":true},"cell_type":"code","source":"def construct_feature_columns():\n        \n    engineered_features = []\n    \n    for continuous_feature in FEATURES:\n        engineered_features.append(\n            tf.contrib.layers.real_valued_column(continuous_feature))\n\n    for categorical_feature in FEATURES_CAT:\n        sparse_column = tf.contrib.layers.sparse_column_with_hash_bucket(\n            categorical_feature, hash_bucket_size=1000)    \n        \n    engineered_features.append(tf.contrib.layers.embedding_column(sparse_id_column=sparse_column, dimension=16,combiner=\"sum\"))\n    \n    feature_columns = set(engineered_features)\n    \n    return feature_columns","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"ef907f3b-ed53-4395-b706-53c75ba2f218","_uuid":"742ddb27fcd592b8b897bd1b3ec4d18869cea0d4","collapsed":true,"trusted":true},"cell_type":"code","source":"def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    \n    \"\"\"\n    Args:\n      features: pandas DataFrame of features\n      targets: pandas DataFrame of targets\n      batch_size: Size of batches to be passed to the model\n      shuffle: True or False. Whether to shuffle the data.\n      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n    Returns:\n      Tuple of (features, labels) for next data batch\n    \"\"\"\n    \n    # Convert pandas data into a dict of np arrays.\n    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n    \n    # Construct a dataset, and configure batching/repeating\n    ds = Dataset.from_tensor_slices((features,targets)) \n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    # Shuffle the data, if specified\n    if shuffle:\n        ds = ds.shuffle(10000)\n    \n    # Return the next batch of data\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels\n","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"d88871da-c56a-4e31-9ce5-8ff476376f16","_uuid":"4cc95c8484f31e0f8d74cf7d48f939b2cfcf355a","collapsed":true,"trusted":true},"cell_type":"code","source":"def train_model(\n    my_optimizer,\n    value_of_clip,\n    steps,\n    batch_size,\n    hidden_units,\n    training_examples,\n    training_targets,\n    validation_examples,\n    validation_targets ) :\n    \n    periods = 10\n    steps_per_period = steps / periods\n    # Create a linear regressor object.\n\n    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, value_of_clip)\n    \n    dnn_regressor = tf.estimator.DNNRegressor(feature_columns=construct_feature_columns(), \n                                              activation_fn = tf.nn.relu, \n                                              hidden_units= hidden_units, \n                                              optimizer = my_optimizer)\n    \n    # Create input functions\n    training_input_fn = lambda: my_input_fn(training_examples, \n                                            training_targets[\"SalePrice\"], \n                                            batch_size=batch_size)\n    predict_training_input_fn = lambda: my_input_fn(training_examples, \n                                            training_targets[\"SalePrice\"], \n                                            num_epochs=1, \n                                            shuffle=False)\n    predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n                                            validation_targets[\"SalePrice\"], \n                                            num_epochs=1, \n                                            shuffle=False)\n\n    # Train the model, but do so inside a loop so that we can periodically assess\n    # loss metrics.\n    print( \"Training model...\")\n    print (\"RMSE (on training data):\")\n    \n    training_rmse = []\n    validation_rmse = []\n    for period in range (0, periods):\n        # Train the model, starting from the prior state.\n        dnn_regressor.train(\n            input_fn=training_input_fn,\n            steps=steps_per_period\n        )\n        \n        # Take a break and compute predictions.\n        training_predictions = dnn_regressor.predict(input_fn=predict_training_input_fn)\n        training_predictions = np.array([item['predictions'][0] for item in training_predictions])\n        validation_predictions = dnn_regressor.predict(input_fn=predict_validation_input_fn)\n        validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n\n        # Compute training and validation loss.\n        training_root_mean_squared_error = math.sqrt(\n            metrics.mean_squared_error(training_predictions, training_targets))\n        validation_root_mean_squared_error = math.sqrt(\n            metrics.mean_squared_error(validation_predictions, validation_targets))\n        # Occasionally print the current loss.\n        print (\"RMSE on training sample at period %02d : is %0.2f \" % (period, training_root_mean_squared_error))\n        print (\"RMSE on validation sample at period %02d : is  %0.2f \" % (period, validation_root_mean_squared_error))\n        # Add the loss metrics from this period to our list.\n        training_rmse.append(training_root_mean_squared_error)\n        validation_rmse.append(validation_root_mean_squared_error)\n        \n    print( \"Model training finished.\")\n    # Output a graph of loss metrics over periods.\n    plt.ylabel(\"RMSE\")\n    plt.xlabel(\"Periods\")\n    plt.title(\"Root Mean Squared Error vs. Periods\")\n    plt.tight_layout()\n    plt.plot(training_rmse, label=\"training\", color = \"red\", alpha = 0.5) \n    plt.plot(validation_rmse, label=\"validation\", color = \"green\", alpha = 0.5)\n    plt.legend()\n    plt.show()\n    \n    return dnn_regressor","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"b7100eeb-b9fa-4c30-b9a4-13a816db1a6f","_uuid":"613f022d815c566a36fee8d1d0d74bc698c528c7","collapsed":true,"trusted":true},"cell_type":"code","source":"def model_predict():\n    \n    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(tf.train.FtrlOptimizer(learning_rate = 0.0025), 0.3)\n    \n    dnn_regressor = tf.estimator.DNNRegressor(feature_columns=construct_feature_columns(), \n                                                  activation_fn = tf.nn.relu, \n                                                  hidden_units= [2000,2000], \n                                                  optimizer = my_optimizer)\n\n    predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n                                                validation_targets[\"SalePrice\"], \n                                                num_epochs=1, \n                                                shuffle=False)\n\n    dnn_regressor.train(\n        input_fn=predict_validation_input_fn,\n        steps=6\n            )\n    \n    test_predictions = dnn_regressor.predict(input_fn=predict_validation_input_fn)\n    test_predictions = np.array([item['predictions'][0] for item in test_predictions])\n    \n    \n    return test_predictions","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"0dd8a0c5-7b3f-4783-9311-0f819090f4d1","_uuid":"69afb82bb40c2b54e1e07a622a3dfd1bf1152145","collapsed":true,"trusted":true},"cell_type":"code","source":"def submit() :\n    estimator_predictions = model_predict()\n    final_predictions = estimator_predictions\n    submission = pd.DataFrame()\n    submission['Id'] = test_ID\n    submission['SalePrice'] = final_predictions\n    submission.to_csv('submission.csv',index=False)","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"a3857245-ad9d-46df-9624-f0165cc38385","_uuid":"5eea4866ff3e37a02c6ed64a51f58a860d75e222"},"cell_type":"markdown","source":"####  parameters ajustement is the result of some trial by myself"},{"metadata":{"_cell_guid":"8464c15b-e2a0-425d-9c8b-b0ddff788c48","_uuid":"c3093d2144f8a5cd8c589ad50c6f2c23e7c643c8","trusted":true},"cell_type":"code","source":"def main() : \n    \n    training_examples, validation_examples, training_targets, validation_targets =  train_test_split(\n        preprocess_features(full_df),preprocess_target(full_df), test_size = 0.5, random_state = 1234)\n    \n    train_model(\n    my_optimizer=tf.train.FtrlOptimizer(learning_rate=0.0025),\n    value_of_clip = 0.3,\n    steps=2000,\n    batch_size=10,\n    hidden_units=[2000,2000],\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\n    \n    submit()\n\nif __name__ == '__main__' :\n    main()","execution_count":22,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9b67c0cd482a48708ced0161a8c9f6a7f54dd1d1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9e2d79eb902b7d4fe52afe34b527cf73bf4683a7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}